{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75cde01f",
   "metadata": {},
   "source": [
    "# 3. Results Visualization and Analysis\n",
    "\n",
    "**Objective:** Load the final evaluation results generated by `evaluate.py`, create comparative visualizations, and perform statistical analysis to validate the thesis hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78856d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8acb3",
   "metadata": {},
   "source": [
    "### Step 1: Parse Evaluation Results from Log Files\n",
    "\n",
    "We will read all `evaluation_results_...txt` files from the `../results` directory and extract the key performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eval_file(filepath):\n",
    "    metrics = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Model:' in line:\n",
    "                metrics['Model'] = line.split(':', 1)[1].strip()\n",
    "            elif ':' in line:\n",
    "                key, val = line.split(':', 1)\n",
    "                try:\n",
    "                    metrics[key.strip()] = float(re.findall(r\"[\\d\\.]+\", val)[0])\n",
    "                except IndexError:\n",
    "                    continue\n",
    "    return metrics\n",
    "\n",
    "results_dir = '../results/'\n",
    "eval_files = [os.path.join(results_dir, f) for f in os.listdir(results_dir) if f.startswith('evaluation_results_') and f.endswith('.txt')]\n",
    "\n",
    "results_data = [parse_eval_file(f) for f in eval_files]\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values(by='F1-Score', ascending=False)\n",
    "\n",
    "print(\"--- Final Model Performance Summary ---\")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3abf3",
   "metadata": {},
   "source": [
    "### Step 2: Visualize Performance Comparison\n",
    "\n",
    "Create bar charts to visually compare the key performance metrics across all models. This is a crucial visualization for Chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f19397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df, metrics_to_plot):\n",
    "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(18, 6), sharey=True)\n",
    "    fig.suptitle('Model Performance Comparison on Test Set', fontsize=16)\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        sns.barplot(ax=axes[i], data=df, x='Model', y=metric, palette='viridis')\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        for container in axes[i].containers:\n",
    "            axes[i].bar_label(container, fmt='%.4f')\n",
    "    \n",
    "    axes[0].set_ylabel('Score')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "if not df_results.empty:\n",
    "    plot_metrics(df_results, ['Accuracy', 'Precision', 'Recall', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ba7f4",
   "metadata": {},
   "source": [
    "### Step 3: Display Confusion Matrices\n",
    "\n",
    "Load and display the confusion matrix images generated by `evaluate.py` for a side-by-side qualitative comparison of error patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_files = sorted([os.path.join(results_dir, f) for f in os.listdir(results_dir) if f.startswith('confusion_matrix_') and f.endswith('.png')])\n",
    "\n",
    "if cm_files:\n",
    "    num_files = len(cm_files)\n",
    "    fig, axes = plt.subplots(1, num_files, figsize=(5 * num_files, 5))\n",
    "    if num_files == 1:\n",
    "        axes = [axes] # Make it iterable for a single plot\n",
    "    fig.suptitle('Confusion Matrices on Test Set', fontsize=16)\n",
    "\n",
    "    for ax, file in zip(axes, cm_files):\n",
    "        img = Image.open(file)\n",
    "        model_name = os.path.basename(file).replace('confusion_matrix_','').replace('.png', '')\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(model_name.replace('_', ' ').title())\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
